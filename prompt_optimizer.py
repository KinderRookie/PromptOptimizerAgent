import os
import json
import math
import time
import random
from typing import TypedDict, Any, Literal

import pandas as pd
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import START, END, StateGraph

# Configuration
SAMPLE_SIZE = 20  # Number of samples to evaluate on

class OptimizerState(TypedDict, total=False):
    prompt: str
    best_prompt: str
    best_score: float
    attempts: list[dict[str, Any]]
    iter: int
    max_iters: int
    samples: list[dict[str, Any]]
    _last_predictions_runs: list[list[str]]


def load_dataset(csv_path: str, sample_size: int = SAMPLE_SIZE, seed: int | None = None) -> list[dict[str, Any]]:
    print(f"ğŸ“Š CSV íŒŒì¼ ë¡œë“œ ì¤‘: {csv_path}")
    # Handle complex CSV with multiline content
    df = pd.read_csv(csv_path, on_bad_lines='skip')
    print(f"âœ… CSV ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰ ë°œê²¬")
    
    if len(df) < sample_size:
        raise ValueError(f"Dataset has only {len(df)} rows, but sample_size={sample_size}")
    
    if seed is not None:
        print(f"ğŸ¯ {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘ (seed={seed})")
    else:
        print(f"ğŸ¯ {sample_size}ê°œ ëœë¤ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘")
    if seed is not None:
        sampled = df.sample(n=sample_size, random_state=seed).reset_index(drop=True)
    else:
        sampled = df.sample(n=sample_size).reset_index(drop=True)
    rows: list[dict[str, Any]] = []
    for _, r in sampled.iterrows():
        rows.append({
            "title": str(r.get("title", "")),
            "content": str(r.get("content", "")),
            "label": str(r.get("label", "")).strip(),
        })
    print(f"âœ… ìƒ˜í”Œ ì¤€ë¹„ ì™„ë£Œ: {len(rows)}ê°œ")
    return rows


DEFAULT_BASE_PROMPT = (
    """
ë‹¹ì‹ ì€ ë§¤ìš° ì—„ê²©í•˜ê³  ì •í™•í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ìœ ì¼í•œ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œê°€ 'ìë™ì°¨' ìì²´(ì˜ˆ: ì‹ ì°¨ ì¶œì‹œ, ìë™ì°¨ ê¸°ìˆ , ìë™ì°¨ ì‚°ì—… ë™í–¥)ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì‚¬ì†Œí•œ ì–¸ê¸‰ì´ë‚˜ ê°„ì ‘ì ì¸ ì—°ê´€ì„±ì€ ì „ë¶€ ë¬´ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì˜ íŒë‹¨ì€ ì˜¤ì§ '1' ë˜ëŠ” '0'ìœ¼ë¡œë§Œ ì¶œë ¥ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì–´ë–¤ ë‹¨ì–´ë‚˜ ì„¤ëª…ë„ ì¶”ê°€í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

ì•„ë˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì‹­ì‹œì˜¤.
[ê·œì¹™]
1. í•µì‹¬ ì£¼ì œê°€ ìë™ì°¨ì¸ê°€? ê¸°ì‚¬ì˜ ì œëª©ê³¼ ë³¸ë¬¸ ì „ì²´ë¥¼ ê³ ë ¤í–ˆì„ ë•Œ, í•µì‹¬ ì£¼ì œê°€ ëª…ë°±í•˜ê²Œ ìë™ì°¨, ìë™ì°¨ ì‚°ì—…, ìë™ì°¨ ê¸°ìˆ , ì‹ ì°¨ ë¦¬ë·° ë“±ì´ì–´ì•¼ë§Œ '1'ì…ë‹ˆë‹¤.
2. ìë™ì°¨ì˜ ë‹¨ìˆœ ì–¸ê¸‰ì€ '0'ì…ë‹ˆë‹¤. êµí†µì‚¬ê³ , êµí†µ ì²´ì¦, íŠ¹ì • ì¸ë¬¼ì´ ì°¨ë¥¼ íƒ”ë‹¤ëŠ” ë‚´ìš©, ë°°ê²½ì— ìë™ì°¨ê°€ ë“±ì¥í•˜ëŠ” ì‚¬ê±´ ë“±ì€ ìë™ì°¨ê°€ í•µì‹¬ ì£¼ì œê°€ ì•„ë‹ˆë¯€ë¡œ '0'ì…ë‹ˆë‹¤.
3. ì£¼ì œê°€ ëª¨í˜¸í•˜ë©´ '0'ì…ë‹ˆë‹¤. ìë™ì°¨ ì™¸ì— ë‹¤ë¥¸ ì£¼ì œ(ì˜ˆ: ê²½ì œ, ê¸°ìˆ , ì‚¬íšŒ)ì™€ ë™ë“±í•œ ë¹„ì¤‘ìœ¼ë¡œ ë‹¤ë¤„ì§€ê±°ë‚˜, ìë™ì°¨ê°€ ë‹¨ì§€ ì˜ˆì‹œë¡œ ì‚¬ìš©ëœ ê²½ìš° ë¬´ì¡°ê±´ '0'ì…ë‹ˆë‹¤.
4. ì¶œë ¥ì€ ì˜¤ì§ '1' ë˜ëŠ” '0'ì…ë‹ˆë‹¤. ì–´ë– í•œ ê²½ìš°ì—ë„ ë‹¤ë¥¸ ì„¤ëª…, ë¬¸ì¥, ì£¼ì„ì„ í¬í•¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
    """
    .strip()
)


def format_news_for_user(row: dict[str, Any]) -> str:
    return f"[ê¸°ì‚¬]\nì œëª©: {row['title']}\në‚´ìš©: {row['content']}"


def parse_label(text: str) -> str:
    t = (text or "").strip()
    if t.startswith("```") and t.endswith("```"):
        t = t.strip("`")
    # Normalize to just '0' or '1'
    if "1" in t and "0" not in t:
        return "1"
    if "0" in t and "1" not in t:
        return "0"
    # Fallback: take first char if valid
    if t[:1] in {"0", "1"}:
        return t[:1]
    return "0"


def classify_once(eval_llm: ChatOpenAI, prompt: str, row: dict[str, Any]) -> str:
    msgs = [
        SystemMessage(content=prompt),
        HumanMessage(content=format_news_for_user(row)),
    ]
    res = eval_llm.invoke(msgs)
    if isinstance(res, AIMessage):
        return parse_label(res.content)
    return "0"


def evaluate_prompt(
    eval_llm: ChatOpenAI,
    prompt: str,
    samples: list[dict[str, Any]],
    runs: int = 3,
) -> dict[str, Any]:
    print(f"ğŸ” í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹œì‘ ({runs}íšŒ ìˆ˜í–‰)")
    accuracy_runs: list[float] = []
    predictions_runs: list[list[str]] = []
    for run_idx in range(runs):
        print(f"  ğŸ“ {run_idx+1}/{runs}ë²ˆì§¸ í‰ê°€ ì¤‘...")
        preds: list[str] = []
        correct = 0
        for i, row in enumerate(samples):
            yhat = classify_once(eval_llm, prompt, row)
            preds.append(yhat)
            if yhat == str(row["label"]).strip():
                correct += 1
            print(f"    ìƒ˜í”Œ {i+1}: ì˜ˆì¸¡={yhat}, ì •ë‹µ={row['label']}, {'âœ…' if yhat == str(row['label']).strip() else 'âŒ'}")
        acc = correct / len(samples)
        accuracy_runs.append(acc)
        predictions_runs.append(preds)
        print(f"  ğŸ¯ {run_idx+1}íšŒì°¨ ì •í™•ë„: {acc:.3f}")

    accuracy = sum(accuracy_runs) / len(accuracy_runs)
    L = len(prompt)
    # score favors accuracy, lightly penalizes very long prompts
    length_term = math.sqrt(max(0.0, 1.0 - (L / 3000.0) ** 2))
    score = 0.9 * accuracy + 0.1 * length_term
    print(f"ğŸ“Š í‰ê·  ì •í™•ë„: {accuracy:.3f}, ìµœì¢… ì ìˆ˜: {score:.3f}")

    return {
        "accuracy": accuracy,
        "score": score,
        "accuracy_runs": accuracy_runs,
        "predictions_runs": predictions_runs,
    }


def aggregate_errors(samples: list[dict[str, Any]], predictions_runs: list[list[str]]) -> list[dict[str, Any]]:
    # Collect misclassified cases across runs (use majority mistake indicator)
    errors: list[dict[str, Any]] = []
    n = len(samples)
    runs = len(predictions_runs)
    for i in range(n):
        votes = [predictions_runs[r][i] for r in range(runs)]
        # If any run misclassified, include for analysis
        gold = str(samples[i]["label"]).strip()
        if any(v != gold for v in votes):
            errors.append({
                "title": samples[i]["title"],
                "content": samples[i]["content"],
                "label": gold,
                "pred_votes": votes,
            })
    # Limit to keep the improve prompt short
    return errors[:8]


def improve_prompt_with_llm(
    llm: ChatOpenAI,
    current_prompt: str,
    best_prompt: str,
    best_score: float,
    errors: list[dict[str, Any]],
) -> str:
    sys = SystemMessage(
        content=(
            "ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. ì•„ë˜ ë¶„ë¥˜ íƒœìŠ¤í¬ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë” ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ê°œì„ í•´ ì£¼ì„¸ìš”.\n"
            "ë°˜ë“œì‹œ 'ì¶œë ¥ì€ ì˜¤ì§ 0 ë˜ëŠ” 1' ê·œìœ¨ì„ ìœ ì§€í•˜ê³ , ì˜¤ë¶„ë¥˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ê°„ê²°í•œ ê·œì¹™ì„ ë³´ê°•í•˜ì„¸ìš”.\n"
            "ë„ˆë¬´ ì¥í™©í•œ ë¬¸ì¥ì€ í”¼í•˜ê³ , ê¸ˆì§€/í—ˆìš© ê¸°ì¤€ì„ ëª…í™•íˆ ì •ì œí•˜ì„¸ìš”.\n"
            "ì‘ë‹µì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ, keyëŠ” improved_prompt í•˜ë‚˜ë§Œ í¬í•¨í•˜ì„¸ìš”."
            "ì ìˆ˜ì˜ ê³„ì‚° ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: score = 0.9 * accuracy + 0.1 * sqrt(max(0, 1 - (L/3000)^2)), ì—¬ê¸°ì„œ Lì€ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ì…ë‹ˆë‹¤.\n" \
            "ë‹¹ì‹ ì˜ ëª©í‘œëŠ” accuracyë¥¼ ë†’ì´ë˜, í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n" \
            "ì•„ë˜ëŠ” í˜„ì¬ í”„ë¡¬í”„íŠ¸, ìµœê³  í”„ë¡¬í”„íŠ¸, ìµœê³  ì ìˆ˜, ê·¸ë¦¬ê³  ìµœê·¼ í‰ê°€ì—ì„œì˜ ì˜¤ë¶„ë¥˜ ì‚¬ë¡€ì…ë‹ˆë‹¤.\n" \
            "ì˜¤ë¶„ë¥˜ ì‚¬ë¡€ëŠ” ë°˜ë“œì‹œ ë¶„ì„í•˜ì—¬ ê°œì„ ì— ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤." \
            "\n\n"
        )
    )
    user = HumanMessage(
        content=json.dumps({
            "current_prompt": current_prompt,
            "best_prompt": best_prompt,
            "best_score": round(best_score, 5),
            "common_errors": errors,
        }, ensure_ascii=False)
    )
    res = llm.invoke([sys, user])
    text = res.content if isinstance(res, AIMessage) else "{}"
    try:
        obj = json.loads(text)
        improved = str(obj.get("improved_prompt", "")).strip()
        if improved:
            return improved
    except Exception:
        pass
    # Fallback heuristic: append a brief refinement
    return (
        current_prompt
        + "\n\n[ì¶”ê°€ ê·œì¹™]\n5. ìë™ì°¨ 'ê´€ì„¸/ì •ì±…/ì£¼ê°€/íˆ¬ì' ë“± ë©”íƒ€ ì£¼ì œëŠ” ìë™ì°¨ ìì²´ê°€ ì•„ë‹ˆë¯€ë¡œ 0ìœ¼ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤."
    )


def evaluate_node(state: OptimizerState, eval_llm: ChatOpenAI) -> OptimizerState:
    print(f"\nğŸš€ === ë°˜ë³µ {state.get('iter', 0) + 1} - í‰ê°€ ë‹¨ê³„ ===")
    
    # Generate new samples for this iteration
    csv_path = os.path.join(os.path.dirname(__file__), "final.csv")
    new_samples = load_dataset(csv_path)
    state["samples"] = new_samples
    print(f"ğŸ² ìƒˆë¡œìš´ ëœë¤ ìƒ˜í”Œ ìƒì„±")
    
    eval_result = evaluate_prompt(eval_llm, state["prompt"], state["samples"], runs=1)
    attempt = {
        "iteration": state.get("iter", 0),
        "prompt": state["prompt"],
        "accuracy": eval_result["accuracy"],
        "score": eval_result["score"],
        "accuracy_runs": eval_result["accuracy_runs"],
    }
    state.setdefault("attempts", []).append(attempt)
    
    current_best = state.get("best_score", -1)
    if eval_result["score"] > current_best:
        print(f"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜! {current_best:.3f} â†’ {eval_result['score']:.3f}")
        state["best_score"] = eval_result["score"]
        state["best_prompt"] = state["prompt"]
    else:
        print(f"ğŸ“ˆ í˜„ì¬ ì ìˆ˜: {eval_result['score']:.3f} (ìµœê³ : {current_best:.3f})")
    
    # Save progress after each evaluation
    print("ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì¤‘...")
    save_current_progress(state)
    
    # store last predictions to guide improvement
    print(f"ğŸ” í‰ê°€ ê²°ê³¼ predictions_runs ê¸¸ì´: {len(eval_result['predictions_runs'])}")
    state["_last_predictions_runs"] = eval_result["predictions_runs"]
    return state


def improve_node(state: OptimizerState, improve_llm: ChatOpenAI) -> OptimizerState:
    print(f"\nğŸ”§ === ê°œì„  ë‹¨ê³„ ===")
    preds = state.get("_last_predictions_runs", [])  # type: ignore[assignment]
    print(f"ğŸ” ë””ë²„ê¹…: predictions_runs ê¸¸ì´={len(preds)}")
    if preds:
        print(f"ğŸ” ì²« ë²ˆì§¸ run ì˜ˆì¸¡: {preds[0]}")
        samples = state["samples"]
        print(f"ğŸ” ì‹¤ì œ ë¼ë²¨: {[str(s['label']).strip() for s in samples]}")
    
    errors = aggregate_errors(state["samples"], preds) if preds else []
    print(f"âŒ ë¶„ì„ëœ ì˜¤ë¥˜ ì¼€ì´ìŠ¤: {len(errors)}ê°œ")
    
    if errors:
        print("ğŸ“‹ ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ìƒì„¸:")
        for i, err in enumerate(errors[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"  {i+1}. ì œëª©: {err['title'][:50]}...")
            print(f"     ì˜ˆì¸¡: {err['pred_votes']}, ì •ë‹µ: {err['label']}")
    
    print("ğŸ¤– í”„ë¡¬í”„íŠ¸ ê°œì„  ì¤‘...")
    improved = improve_prompt_with_llm(
        llm=improve_llm,
        current_prompt=state["prompt"],
        best_prompt=state.get("best_prompt", state["prompt"]),
        best_score=float(state.get("best_score", 0.0)),
        errors=errors,
    )
    
    print(f"âœï¸ í”„ë¡¬í”„íŠ¸ ê°œì„  ì™„ë£Œ (ê¸¸ì´: {len(state['prompt'])} â†’ {len(improved)})")
    print(f"\nğŸ“ === ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ===")
    print(improved)
    print("=" * 50)
    
    state["prompt"] = improved
    state["iter"] = int(state.get("iter", 0)) + 1
    return state


def should_continue(state: OptimizerState) -> Literal["improve", "evaluate", "__end__"]:
    # Check if last evaluation was perfect (accuracy = 1.0)
    attempts = state.get("attempts", [])
    if attempts:
        last_attempt = attempts[-1]
        last_accuracy = last_attempt.get("accuracy", 0.0)
        if last_accuracy >= 1.0:
            print("ğŸ¯ ì™„ë²½í•œ ì •í™•ë„ ë‹¬ì„±! ê°œì„  ê±´ë„ˆë›°ê³  ë‹¤ìŒ í‰ê°€ë¡œ...")
            # Increment iter and go to next evaluation
            state["iter"] = int(state.get("iter", 0)) + 1
            # Check if we've reached max iterations after incrementing
            if int(state.get("iter", 0)) >= int(state.get("max_iters", 5)):
                print("âœ… ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬, ì¢…ë£Œ")
                return END
            return "evaluate"
    
    # Normal case: check iterations before improvement
    if int(state.get("iter", 0)) >= int(state.get("max_iters", 5)):
        return END
    return "improve"


def save_history(history_path: str, run_record: dict[str, Any]) -> None:
    os.makedirs(os.path.dirname(history_path), exist_ok=True)
    existing: list[dict[str, Any]] = []
    if os.path.exists(history_path):
        try:
            with open(history_path, "r", encoding="utf-8") as f:
                existing = json.load(f)
        except Exception:
            existing = []
    existing.append(run_record)
    # Sort by best_score descending for convenience
    existing.sort(key=lambda x: x.get("best_score", 0.0), reverse=True)
    with open(history_path, "w", encoding="utf-8") as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)


def save_current_progress(state: OptimizerState) -> None:
    """Save current progress during execution"""
    run_id = int(time.time())
    out_dir = os.path.join(os.path.dirname(__file__), "experiments")
    os.makedirs(out_dir, exist_ok=True)
    
    # Save intermediate record
    record = {
        "run_id": run_id,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(run_id)),
        "status": "in_progress",
        "current_iteration": state.get("iter", 0),
        "best_score": state.get("best_score"),
        "best_prompt": state.get("best_prompt"),
        "attempts": state.get("attempts", []),
        "sample_size": 10,
        "eval_runs": 1,
        "eval_model": "gpt-4o-mini",
        "improve_model": "gpt-5",
    }
    
    # Save to intermediate file
    progress_path = os.path.join(out_dir, "current_progress.json")
    with open(progress_path, "w", encoding="utf-8") as f:
        json.dump(record, f, ensure_ascii=False, indent=2)


def load_best_prompt_from_history(history_path: str) -> str | None:
    if not os.path.exists(history_path):
        return None
    try:
        with open(history_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # Expecting a list of run records
        if isinstance(data, list) and data:
            # Get run with max best_score
            best_run = max(
                (r for r in data if isinstance(r, dict)),
                key=lambda r: r.get("best_score", float("-inf")),
            )
            bp = best_run.get("best_prompt")
            if isinstance(bp, str) and bp.strip():
                return bp
            # Fallback: scan attempts in best_run
            attempts = best_run.get("attempts", []) if isinstance(best_run, dict) else []
            best_attempt = None
            best_score = float("-inf")
            for a in attempts:
                if not isinstance(a, dict):
                    continue
                s = a.get("score", None)
                p = a.get("prompt", None)
                if isinstance(s, (int, float)) and isinstance(p, str) and s > best_score:
                    best_score = float(s)
                    best_attempt = p
            if best_attempt:
                return best_attempt
        return None
    except Exception:
        return None


def main() -> None:
    print("ğŸ¯ === í”„ë¡¬í”„íŠ¸ ìµœì í™” ì—ì´ì „íŠ¸ ì‹œì‘ ===\n")
    
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY not set in environment")

    print("ğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    # Evaluation model: gpt-4o-mini for classification
    eval_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.4, openai_api_key=api_key)
    # Improvement model: gpt-5 for prompt optimization
    improve_llm = ChatOpenAI(model="gpt-5", temperature=0.2, openai_api_key=api_key)
    print("âœ… í‰ê°€ìš©: gpt-4o-mini, ê°œì„ ìš©: gpt-5")

    # Samples will be generated fresh for each iteration

    # Build LangGraph
    def _evaluate(state: OptimizerState) -> OptimizerState:
        return evaluate_node(state, eval_llm)

    def _improve(state: OptimizerState) -> OptimizerState:
        return improve_node(state, improve_llm)

    builder = StateGraph(OptimizerState)
    builder.add_node("evaluate", _evaluate)
    builder.add_node("improve", _improve)
    builder.add_edge(START, "evaluate")
    builder.add_conditional_edges("evaluate", should_continue)
    builder.add_edge("improve", "evaluate")
    graph = builder.compile()

    # Load highest-scoring prompt from history if available
    out_dir = os.path.join(os.path.dirname(__file__), "experiments")
    history_path = os.path.join(out_dir, "prompts_history.json")
    print(f"ğŸ“š ì´ì „ ì‹¤í—˜ ê¸°ë¡ í™•ì¸ ì¤‘: {history_path}")
    best_from_history = load_best_prompt_from_history(history_path)
    if best_from_history:
        print("âœ… ì´ì „ ìµœê³  ì ìˆ˜ í”„ë¡¬í”„íŠ¸ ë°œê²¬, ì´ë¥¼ ì‹œì‘ì ìœ¼ë¡œ ì‚¬ìš©")
    else:
        print("ğŸ“ ê¸°ë¡ ì—†ìŒ, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ì‹œì‘")
    initial_prompt = best_from_history or DEFAULT_BASE_PROMPT
    init_state: OptimizerState = {
        "prompt": initial_prompt,
        "best_prompt": initial_prompt,
        "best_score": -1.0,
        "attempts": [],
        "iter": 0,
        "max_iters": 5,  # 5 improvements per run
        "samples": [],  # Will be populated in each evaluation
    }

    # Run graph
    print(f"\nğŸ LangGraph ìµœì í™” ì‹œì‘ (ìµœëŒ€ {init_state['max_iters']}íšŒ ë°˜ë³µ)")
    final_state = graph.invoke(init_state)

    # Prepare final run record
    run_id = int(time.time())
    record = {
        "run_id": run_id,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(run_id)),
        "status": "completed",
        "best_score": final_state.get("best_score"),
        "best_prompt": final_state.get("best_prompt"),
        "attempts": final_state.get("attempts", []),
        "sample_indices_seed": 42,
        "sample_size": 10,
        "eval_runs": 1,
        "eval_model": "gpt-4o-mini",
        "improve_model": "gpt-5",
    }

    # Save final artifacts
    print(f"\nğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘...")
    os.makedirs(out_dir, exist_ok=True)
    save_history(history_path, record)
    
    # Clean up intermediate progress file
    progress_path = os.path.join(out_dir, "current_progress.json")
    if os.path.exists(progress_path):
        os.remove(progress_path)
        print("ğŸ§¹ ì¤‘ê°„ ì§„í–‰ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")

    best_prompt_path = os.path.join(out_dir, f"best_prompt_{run_id}.txt")
    with open(best_prompt_path, "w", encoding="utf-8") as f:
        f.write(str(final_state.get("best_prompt", "")))

    # Console summary
    print("\nğŸ‰ === í”„ë¡¬í”„íŠ¸ ìµœì í™” ì™„ë£Œ ===")
    print(f"ğŸ† ìµœê³  ì ìˆ˜: {final_state.get('best_score'):.4f}")
    print(f"ğŸ”„ ì´ ì‹œë„: {len(final_state.get('attempts', []))}íšŒ")
    print(f"ğŸ“‚ íˆìŠ¤í† ë¦¬: {history_path}")
    print(f"ğŸ“„ ìµœê³  í”„ë¡¬í”„íŠ¸: {best_prompt_path}")
    print("\nì™„ë£Œ! ğŸ¯")


if __name__ == "__main__":
    main()


